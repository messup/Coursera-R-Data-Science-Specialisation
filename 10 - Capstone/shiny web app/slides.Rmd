---
title: "Capstone Project\nWord Prediction Summary"
author: "Alex Van Russelt"
date: "11 March 2018"
output: 
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## 1. Introduction
These slides describe a predictive text product for US English, developed for the Coursera data science capstone project. It summarises how the predictive model works, its performance, and how the product is used.

As of March 2018, the product is hosted here: https://neo9001.shinyapps.io/word_prediction/

## 2. How the model works
The model uses 90% of the provided text data to build the model. The remaining 10% was used to evaluate the model's performance.

The model relies on "n-grams", a contiguous sequence of *n* words taken from a sample of text. For example, the sentence *"The cat sat on the mat"* has three unique 3-grams: *"The cat sat"*, *"cat sat on"*, and *"on the mat"*. Every unique 1-gram, 2-gram, 3-gram and 4-gram in the training data was collated into a frequency table for use in the predictive model.

The model uses Katz's back-off model, which is expressed mathematically on the following slide. The model aims to estimate the conditional probability of a word given its history in the n-gram. The model will initially attempt to predict the next word by looking at the sequence of the final three words in the sentence and comparing them to the 4-grams in the training data that appeared most frequently. If this is not possible (if word sequence observed in the training set, or if the input string has fewer than three words), then the model "backs-off" to 3-grams and attempts to do the same. If required, the model will back-off further to even smaller n-grams.

## 3. How the model works

Katz's back-off model estimates the conditional probabily $P_{bo}$ as:
$$
\begin{align}
& P_{bo} (w_i \mid w_{i-n+1} \cdots w_{i-1}) \\[4pt]
= {} & \begin{cases}
    d_{w_{i-n+1} \cdots w_{i}} \dfrac{C(w_{i-n+1} \cdots w_{i-1}w_{i})}{C(w_{i-n+1} \cdots w_{i-1})} & \text{if } C(w_{i-n+1} \cdots w_i) > k \\[10pt]
    \alpha_{w_{i-n+1} \cdots w_{i-1}} P_{bo}(w_i \mid w_{i-n+2} \cdots w_{i-1}) & \text{otherwise}
\end{cases}
\end{align}
$$
where:

* $C(x)$ = number of times x appears in training.
* $w_i$ = ith word in the given context.
* $d$ is the amount of discounting found by Goodâ€“Turing estimation.
* $\alpha$ is the back-off weight as determined by the leftover probability mass for the (n-1)-gram.
* $k$ was chosen to be equal to 0.

## 4. Predictive performance

The model returns three word predictions. The performance of the model was evaluated using the unseen 10% of the data.

* The first prediction was correct 20% of the time.
* The second prediction was correct 6% of the time.
* The third prediction was correct 3.5% of the time.

All together, the top three results contained the correct word 29% of the time.

## 5. Using the product

The product is used by entering some text into the *Text input* box. The model presents three suggestions to the right hand side under the *Predicted words* column, along with a *Word scores* column which indicates how confident the model.

If the input text does not end in a space, then the model assumes that the final word is incomplete, and ignores it when predicting the next word.

Each predicted word is a button that the user is able to click to add a desired word to the end of the text.
